---
title: 'Privatization of AI research: Preprocessing'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: hide
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
# Knitr options
### Generic preamble
rm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.
gc() #free up memrory and report the memory usage.
graphics.off()

Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Load packages
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr) # For extra-piping operators (eg. %<>%)

# UML
#library(FactoMineR)
#library(factoextra)

# Descriptives
#library(skimr)
#library(stargazer)

# Viz
#library(patchwork)
```

# PApers & Conceps

## Read data

```{r}
data <- read_csv('../../input/works.csv') %>%
  mutate(id = id %>% as.character()) %>%
  select(-title, -issn)
```

```{r}
data %>% glimpse()
```

## Extract citations, novelty, OA

```{r}
indicators <- data %>% select(list_aid, year, list_aid, oa, cited_by_count, novelty_lee) %>%
  mutate(list_aid = list_aid %>% str_split('\n')) %>% 
  unnest(list_aid) %>%
  rename(AID = list_aid) %>%
  group_by(AID, year) %>%
  summarise(
    paper_n = n(),
    citation_n = cited_by_count %>% sum(na.rm = TRUE),
    citation_mean = cited_by_count %>% mean(na.rm = TRUE),
    citation_max = cited_by_count %>% max(),
    oa_mean = oa %>% mean(),
    novelty_mean = novelty_lee %>% mean(na.rm = TRUE),
    # novelty_max = novelty_lee %>% max()
  ) %>%
  ungroup()
```

```{r}
indicators %>% write_rds('../../temp/author_indicators.rds')
```

```{r}
rm(indicators)
```


## Extract concepts

```{r}
concepts <- data %>% select(id, year, list_aid, concepts) %>%
    mutate(concepts = concepts %>% str_split('\n')) %>% 
    unnest(concepts)
```

```{r}
rm(data)
```


```{r}
concepts %>% count(concepts, sort = TRUE)
```

### Concept reduction

```{r}
# Define the number of fields to be included
n_concepts <- 100
n_start = 1

# Extract main concepts
concepts_main <- concepts %>% count(concepts, sort = TRUE) %>%
  slice(n_start:(n_start + n_concepts))
```

```{r}
#Update author dyn
concepts %<>%
  mutate(concepts = ifelse(concepts %in% (concepts_main %>% pull(concepts)), concepts, 'others'))
```

### Main Concepts 

```{r}
# GEneral concepts without informative value
concept_drop <- tibble(
  concepts =  c('computer science', 'artificial intelligence', 'machine learning')
)
```

```{r}
# Alternative 1: only 1 concept per author -> Q: Maybe author/year
concepts_1 <- concepts %>%
  anti_join(concept_drop, by = 'concepts')
```

```{r}
concepts %>% write_rds('../../temp/paper_concepts.rds')
```


```{r}
rm(concept_drop, concepts_main, n_start, n_concepts,
   concepts)
```


```{r}
 concepts_1 %<>%
  mutate(list_aid = list_aid %>% str_split('\n')) %>% 
  unnest(list_aid) %>%
  rename(AID = list_aid) 
```

```{r}
concepts_1 %<>%
  count(AID, year, concepts) 
```

```{r}
concepts_1 %<>%
  group_by(AID, year) %>% 
  slice_max(order_by = n, n = 1, with_ties = FALSE) %>% 
  ungroup() %>%
  select(-n)
```


```{r}
## Faster but not working data table alternative
#data.table::setDT(concepts_1)[, .SD[which.max(n)], by=list(AID, year)] %>%
#  data.table::setDF() %>%
#  as_tibble() %>%
#  select(AID, year, concepts) %>%
#  arrange(AID, year) %>%
#  distinct(AID, year, .keep_all = TRUE)
```

```{r}
concepts_1 %>% head()
```


```{r}
concepts_1 %>% write_rds('../../temp/author_concepts.rds')
```

<!--- 
### Concept clusters (alternative solution, work in progress, not really good results yet)

```{r}
concepts_wide <- concepts %>%
  select(id, concepts) %>%
  count(id, concepts) %>%
  pivot_wider(names_from = concepts, values_from = n, values_fill = 0, names_prefix = 'con_')
```

```{r}
res_pca <- concepts_wide %>% 
  column_to_rownames(id) %>%
  PCA(scale.unit = FALSE, graph = FALSE)
```

```{r}
res_hcpc <- res_pca %>% 
  HCPC(nb.clust = -1, #  self determined: higher relative loss of inertia
       graph = FALSE) 
```

```{r}
res_km <- concepts_wide  %>% 
  column_to_rownames(id) %>%
  kmeans(centers = 3)  
```
--->

```{r}
# Delete all objects
rm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.
gc() #free up memrory and report the memory usage.
```

# Main data (reearcher panel)

## Load data

```{r}
data <- read_csv('../../input/variables.csv') %>% 
  select(-seniority, -concepts, -gender) %>%
  mutate(AID = AID %>% as.character()) 
```

## first preprocess

```{r}
data %<>% 
  mutate(aff_id = aff_id %>% str_remove('https://openalex.org/'))
```

## Join with othr data

```{r}
# indicators
data %<>% left_join(read_rds('../../temp/author_indicators.rds') %>%
                      select(AID, year, citation_n, oa_mean, novelty_mean),
                    by = c('AID', 'year'))
```

```{r}
# concepts
data %<>% left_join(read_rds('../../temp/author_concepts.rds') %>%
                      select(AID, year, concepts),
                    by = c('AID', 'year'))
```

## Static author

```{r}
data_static <- data %>%
  drop_na(aff_type) %>%
  mutate(aff_uni = aff_type == 'education') %>%
  group_by(AID) %>%
  summarise(year_n = n(),
            year_min = year %>% min(na.rm = TRUE),
            year_max = year %>% max(na.rm = TRUE),
            aff_uni_all = aff_uni %>% min(na.rm = TRUE),
            aff_ind_all = aff_uni %>% max(na.rm = TRUE),
            switcher = switcher %>% max(na.rm = TRUE)
            ) %>%
  ungroup() %>%
  mutate(year_range = year_max - year_min + 1,
         aff_ind_all = 1- aff_ind_all,
         pct_obs = year_range / year_n)
```

Now define the selection criteria for the sample

```{r}
data_static %<>%
  filter(
    year_max >= 2015, # latest year observed
    year_n >= 4, # Number of years observed
    year_range >= 4, # Number of years observed
    pct_obs >= 0.5, # observed at least half of the time
    aff_ind_all == 0 # exclude industry only
    ) 
```

```{r}
data_static %>% write_rds('../../temp/data_author_static.rds')
```

```{r}
data %<>% semi_join(data_static, by = 'AID')
```

```{r}
rm(data_static)
```

## Make complete panel

```{r}
data %<>%   
  group_by(AID) %>%
    complete(year = full_seq(min(year):max(year), 1)) %>%
    arrange(year) %>%
    fill(aff_id, aff_type, transited_t, switcher, seniority_all, .direction = "downup") %>%
  ungroup() 
# TODO: Figure out what to alternatively do with seniority_all. Up to now its not really correct
```

## Replace NAs
  
```{r}
data %<>%
  replace_na(list(DL = 0, transition = 0, transited_t = 0, switcher = 0,
                  deg_cen = 0, deg_cen_comp = 0,
                  paper_n = 0, citation_n = 0, oa_mean = 0, novelty_mean = 0,
                  concepts = 'unknown'))
```

```{r}
#in-between save
data %>% write_rds('../../temp/data_reg_temp.rds')
```

## Field weighted indicators

```{r}
data %<>%
  group_by(concepts, year) %>%
  mutate(citation_fw = citation_n %>% percent_rank(),
         novelty_fw = novelty_mean %>% percent_rank()) %>%
  ungroup()
```


## Institutions

```{r}
data_institutions <- data %>%
  group_by(aff_id, year) %>%
  summarise(author_n = n_distinct(AID),
            aff_paper_n = n(),
            aff_citation_fw = citation_fw %>% mean(),
            aff_novelty_fw = novelty_fw %>% mean()
            ) %>%
  ungroup() %>%
  mutate(across(everything(), ~replace(.x, is.nan(.x), 0)))
```

```{r}
data_institutions %>% write_rds('../../temp/data_institutions.rds')
```

```{r}
data %<>% left_join(data_institutions %>% select(aff_id, year, aff_citation_fw), by = c('aff_id', 'year'))
```

```{r}
rm(data_institutions)
```


## Rolling mean

```{r}
data %<>% arrange(AID, year)
```


```{r}
## NOTE: Not done now
data %>%
  group_by(AID) %>%
  arrange(year) %>%
  mutate(
    across(c(paper_n, citation_n, oa_mean, novelty_mean, deg_cen, deg_cen_comp, novelty_fw, citation_fw), ~ zoo::rollmean(.x, 3, align = "left"))
  )
```

## some other small thingies

...

## Save

```{r}
data %>% write_rds('../../temp/data_reg.rds')
```

# TODOs

TODO preprocessing:
* X join citation & novelty var
* X Make panel over all years
* X replace NAs
* X lag variables
* X Pre-filtering (or maybe better when loading for regrerssion)
* X Add institutional quelity indicators.

TODO others
* Do a static author collection