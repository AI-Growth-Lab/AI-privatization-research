---
title: 'Privatization of AI research: Preprocessing'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: hide
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
# Knitr options
### Generic preamble
rm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.
gc() #free up memrory and report the memory usage.
graphics.off()

Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Load packages
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr) # For extra-piping operators (eg. %<>%)

# UML
#library(FactoMineR)
#library(factoextra)

# Descriptives
#library(skimr)
#library(stargazer)

# Viz
#library(patchwork)
```

# Papers & Conceps

## Read data

```{r}
data <- read_csv('../../input/works.csv') %>%
  mutate(id = id %>% as.character()) %>%
  select(-title, -issn)
```

## Extract citations, novelty, OA

```{r}
indicators <- data %>% select(list_aid, year, list_aid, oa, cited_by_count, novelty_lee) %>%
  mutate(n_author = list_aid %>% str_count('\n'),
         list_aid = list_aid %>% str_split('\n')) %>% 
  unnest(list_aid) %>%
  rename(AID = list_aid) %>%
  group_by(AID, year) %>%
  summarise(
    author_mean = n_author %>% mean(na.rm = TRUE),
    paper_n = n(),
    citation_n = cited_by_count %>% sum(na.rm = TRUE),
    citation_mean = cited_by_count %>% mean(na.rm = TRUE),
    citation_max = cited_by_count %>% max(),
    oa_mean = oa %>% mean(),
    novelty_mean = novelty_lee %>% mean(na.rm = TRUE),
    # novelty_max = novelty_lee %>% max()
  ) %>%
  ungroup()
```

```{r}
indicators %>% write_rds('../../temp/author_indicators.rds')
```

```{r}
rm(indicators)
```


## Extract concepts

```{r}
concepts <- data %>% select(id, year, list_aid, concepts) %>%
    mutate(concepts = concepts %>% str_split('\n')) %>% 
    unnest(concepts)
```

```{r}
rm(data)
```


```{r}
concepts %>% count(concepts, sort = TRUE)
```

### Concept reduction

```{r}
# Define the number of fields to be included
n_concepts <- 100
n_start = 1

# Extract main concepts
concepts_main <- concepts %>% count(concepts, sort = TRUE) %>%
  slice(n_start:(n_start + n_concepts))
```

```{r}
#Update author dyn
concepts %<>%
  mutate(concepts = ifelse(concepts %in% (concepts_main %>% pull(concepts)), concepts, 'others'))
```

### Main Concepts 

```{r}
# GEneral concepts without informative value
concept_drop <- tibble(
  concepts =  c('computer science', 'artificial intelligence', 'machine learning')
)
```

```{r}
# Alternative 1: only 1 concept per author -> Q: Maybe author/year
concepts_1 <- concepts %>%
  anti_join(concept_drop, by = 'concepts')
```

```{r}
concepts %>% write_rds('../../temp/paper_concepts.rds')
```


```{r}
rm(concept_drop, concepts_main, n_start, n_concepts,
   concepts)
```


```{r}
 concepts_1 %<>%
  mutate(list_aid = list_aid %>% str_split('\n')) %>% 
  unnest(list_aid) %>%
  rename(AID = list_aid) 
```

```{r}
concepts_1 %<>%
  count(AID, year, concepts) 
```

```{r}
concepts_1 %<>%
  group_by(AID, year) %>% 
  slice_max(order_by = n, n = 1, with_ties = FALSE) %>% 
  ungroup() %>%
  select(-n)
```


```{r}
## Faster but not working data table alternative
#data.table::setDT(concepts_1)[, .SD[which.max(n)], by=list(AID, year)] %>%
#  data.table::setDF() %>%
#  as_tibble() %>%
#  select(AID, year, concepts) %>%
#  arrange(AID, year) %>%
#  distinct(AID, year, .keep_all = TRUE)
```


```{r}
concepts_1 %>% write_rds('../../temp/author_concepts.rds')
```

<!--- 
### Concept clusters (alternative solution, work in progress, not really good results yet)

```{r}
concepts_wide <- concepts %>%
  select(id, concepts) %>%
  count(id, concepts) %>%
  pivot_wider(names_from = concepts, values_from = n, values_fill = 0, names_prefix = 'con_')
```

```{r}
res_pca <- concepts_wide %>% 
  column_to_rownames(id) %>%
  PCA(scale.unit = FALSE, graph = FALSE)
```

```{r}
res_hcpc <- res_pca %>% 
  HCPC(nb.clust = -1, #  self determined: higher relative loss of inertia
       graph = FALSE) 
```

```{r}
res_km <- concepts_wide  %>% 
  column_to_rownames(id) %>%
  kmeans(centers = 3)  
```
--->

```{r}
# Delete all objects
rm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.
gc() #free up memrory and report the memory usage.
```

# Main data (reearcher panel)

## Load data

```{r}
data <- read_csv('../../input/variables.csv') %>% 
  select(-seniority, -seniority_all, -paper_n, transited_t, -switcher, -concepts, -gender) %>%
  mutate(AID = AID %>% as.character()) 
```

## first preprocess

```{r}
data %<>% 
  mutate(aff_id = aff_id %>% str_remove('https://openalex.org/'))
```

## Static author

```{r}
data_static <- read_csv('../../input/author_static.csv') %>%
  filter(type %in% c('education', 'switcher')) %>%
  mutate(AID = AID %>% as.character()) %>%
  filter(seniority >= 1960)
```

```{r}
# only select edu and switchers
data %<>%
  semi_join(data_static, by = 'AID')
```

```{r}
author_range <- data %>%
  group_by(AID) %>%
  summarise(year_n = n(),
            year_min = year %>% min(na.rm = TRUE),
            year_max = year %>% max(na.rm = TRUE),
            year_transit = max(year * transition, na.rm = TRUE)
            ) %>%
  ungroup() %>%
  mutate(year_transit = ifelse(year_transit == 0, NA, year_transit),
         year_range = year_max - year_min + 1,
         pct_obs = year_range / year_n)
```

```{r}
# Merge with author static
data_static %<>%
  left_join(author_range, by = 'AID')
```


Now define the selection criteria for the sample

```{r}
author_range %<>%
  filter(
    year_min >= 1955,
    year_max >= 2015, # latest year observed
    year_n >= 4, # Number of years observed
    year_range >= 4, # Number of years observed
    pct_obs >= 0.5, # observed at least half of the time
    ) 
```

```{r}
# Filter data
data %<>% semi_join(author_range, by = 'AID')
```

```{r}
rm(author_range)
```


## Join with other data

```{r}
# indicators
data %<>% left_join(read_rds('../../temp/author_indicators.rds') %>%
                      select(AID, year, paper_n, author_mean, citation_n, oa_mean, novelty_mean),
                    by = c('AID', 'year'))
```

```{r}
# concepts
data %<>% left_join(read_rds('../../temp/author_concepts.rds') %>%
                      select(AID, year, concepts),
                    by = c('AID', 'year'))
```

## Make complete panel

```{r}
data %<>%   
  group_by(AID) %>%
    complete(year = full_seq(min(year):max(year), 1)) %>%
    arrange(year) %>%
    fill(aff_id, aff_type, DL, .direction = "downup") %>%
  ungroup() 
```

## Merge with author static

```{r}
data %<>% 
  left_join(data_static %>% select(AID, type, seniority, year_transit, year_max, year_min), by = 'AID')
```

```{r}
data %<>%
  mutate(seniority = year - seniority + 1,
    transited_t = year - year_transit + 1)
```


## Replace NAs
  
```{r}
data %<>%
  replace_na(list(transition = 0, 
                  deg_cen = 0, deg_cen_comp = 0,
                  paper_n = 0, author_mean = 0, citation_n = 0, oa_mean = 0, novelty_mean = 0,
                  concepts = 'unknown'))
```

```{r}
#in-between save
data %>% write_rds('../../temp/data_reg_temp.rds')
```

## Field weighted indicators

```{r}
data %<>%
  group_by(concepts, year) %>%
  mutate(citation_fw = citation_n %>% percent_rank(),
         novelty_fw = novelty_mean %>% percent_rank()) %>%
  ungroup()
```


## Institutions

```{r}
data_institutions <- data %>%
  group_by(aff_id, year) %>%
  summarise(author_n = n_distinct(AID),
            aff_paper_n = n(),
            aff_citation_fw = citation_fw %>% mean(),
            aff_novelty_fw = novelty_fw %>% mean()
            ) %>%
  ungroup() %>%
  mutate(across(everything(), ~replace(.x, is.nan(.x), 0)))
```

```{r}
data_institutions %>% write_rds('../../temp/data_institutions.rds')
```

```{r}
data %<>% left_join(data_institutions %>% select(aff_id, year, aff_citation_fw, aff_novelty_fw), by = c('aff_id', 'year'))
```

```{r}
rm(data_institutions)
```

## Rolling mean

```{r}
data %<>% arrange(AID, year)
```

```{r}
data %<>%
  group_by(AID) %>%
  arrange(year) %>%
  mutate(
    across(c(paper_n, 
             author_mean,
             citation_n, 
             oa_mean, 
             novelty_mean, 
             deg_cen, 
             deg_cen_comp, 
             novelty_fw, 
             citation_fw,
             aff_citation_fw,
             aff_novelty_fw), ~ zoo::rollmean(.x, 3, fill = NA, align = "left"))
  ) %>% 
  ungroup()
```


## some other small thingies

```{r}
# round long floats
data %<>%
  mutate(
    across(where(is_double), round, 3)
  )
```

## Save

```{r}
data %>% write_rds('../../temp/data_reg.rds')
```





# TODOs

